{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a0003e",
   "metadata": {},
   "source": [
    "THIS IS A MATCH PREDICTION MODEL DEPLOYED TO PREDICT BEFORE THE MATCH STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b749cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3daacf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Hyperparameter Tuning for 'Draw-Finder'...\n",
      "Applying SMOTE to the training data...\n",
      "SMOTE complete. New training set size: Counter({1: 5572, 0: 5572})\n",
      "\n",
      "Setting up GridSearchCV for XGBoost...\n",
      "Starting the hyperparameter search... This may take some time.\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "\n",
      "Hyperparameter search complete.\n",
      "Best parameters found:  {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 300}\n",
      "\n",
      "Evaluating the best model found by the search...\n",
      "\n",
      "-------------------------------------------------\n",
      "Tuned Draw-Finder Model Test Accuracy: 74.99%\n",
      "-------------------------------------------------\n",
      "\n",
      "Tuned Draw-Finder Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not-Draw (0)       0.77      0.96      0.85      1394\n",
      "    Draw (1)       0.45      0.09      0.15       453\n",
      "\n",
      "    accuracy                           0.75      1847\n",
      "   macro avg       0.61      0.53      0.50      1847\n",
      "weighted avg       0.69      0.75      0.68      1847\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Initiating Hyperparameter Tuning for 'Draw-Finder'...\")\n",
    "\n",
    "# --- 1. Load and Prepare Data (as before) ---\n",
    "df_final = pd.read_csv(\"full_feature_dataset_expanded.csv\")\n",
    "df_final['is_draw'] = np.where(df_final['FTR'] == 'D', 1, 0)\n",
    "\n",
    "form_feature_names = [col for col in df_final.columns if 'form' in col]\n",
    "h2h_feature_names = [col for col in df_final.columns if 'H2H' in col]\n",
    "odds_feature_names = [col for col in df_final.columns if 'Avg_Odds' in col]\n",
    "final_feature_list = ['HomeTeam', 'AwayTeam','HTHG','HTAG','HS','AS','AST','HST','HC','AC','HY','AY','HR','AR','HF','AF'] + odds_feature_names + form_feature_names + h2h_feature_names\n",
    "\n",
    "X = df_final[final_feature_list].copy()\n",
    "y = df_final['is_draw']\n",
    "\n",
    "X['HomeTeam'] = LabelEncoder().fit_transform(X['HomeTeam'])\n",
    "X['AwayTeam'] = LabelEncoder().fit_transform(X['AwayTeam'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- 2. Apply SMOTE (as before) ---\n",
    "print(\"Applying SMOTE to the training data...\")\n",
    "sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\"SMOTE complete. New training set size:\", Counter(y_train_res))\n",
    "\n",
    "# --- 3. Hyperparameter Tuning with GridSearchCV ---\n",
    "print(\"\\nSetting up GridSearchCV for XGBoost...\")\n",
    "# =cooment\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# We still use scale_pos_weight for imbalanced data\n",
    "scale_pos_weight = np.sum(y_train_res == 0) / np.sum(y_train_res == 1)\n",
    "\n",
    "# Create the XGBoost model instance\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "# We use 'f1' as the scoring metric to optimize for a balance of precision and recall on the 'Draw' class\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=3, # 3-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1 # Use all available CPU cores\n",
    ")\n",
    "\n",
    "print(\"Starting the hyperparameter search... This may take some time.\")\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(\"\\nHyperparameter search complete.\")\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# --- 4. Evaluate the BEST Model ---\n",
    "print(\"\\nEvaluating the best model found by the search...\")\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n-------------------------------------------------\")\n",
    "print(f\"Tuned Draw-Finder Model Test Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"-------------------------------------------------\")\n",
    "print(\"\\nTuned Draw-Finder Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not-Draw (0)', 'Draw (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a9a5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating full 'Draw Specialist' system evaluation...\n",
      "\n",
      "Training Model A (Draw-Finder)...\n",
      "\n",
      "Training Model B (Winner-Picker)...\n",
      "\n",
      "Executing full prediction pipeline on the test set...\n",
      "\n",
      "-------------------------------------------------\n",
      "Final 'Draw Specialist' System Accuracy: 66.54%\n",
      "-------------------------------------------------\n",
      "\n",
      "Final System Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.81      0.82      0.82       540\n",
      "           D       0.29      0.25      0.27       453\n",
      "           H       0.74      0.79      0.76       854\n",
      "\n",
      "    accuracy                           0.67      1847\n",
      "   macro avg       0.61      0.62      0.61      1847\n",
      "weighted avg       0.65      0.67      0.66      1847\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Initiating full 'Draw Specialist' system evaluation...\")\n",
    "\n",
    "# --- 1. Load and Prepare the Full Enriched Dataset ---\n",
    "df_final = pd.read_csv(\"full_feature_dataset1.csv\")\n",
    "\n",
    "# Create the explicit feature list\n",
    "form_feature_names = [col for col in df_final.columns if 'form' in col]\n",
    "h2h_feature_names = [col for col in df_final.columns if 'H2H' in col]\n",
    "odds_feature_names = [col for col in df_final.columns if 'Avg_Odds' in col]\n",
    "final_feature_list = ['HomeTeam', 'AwayTeam'] + odds_feature_names + form_feature_names + h2h_feature_names\n",
    "\n",
    "X = df_final[final_feature_list].copy()\n",
    "y_multi_class = df_final['FTR'] # The original 'A', 'D', 'H' labels\n",
    "\n",
    "# Preprocess categorical features once\n",
    "X['HomeTeam'] = LabelEncoder().fit_transform(X['HomeTeam'])\n",
    "X['AwayTeam'] = LabelEncoder().fit_transform(X['AwayTeam'])\n",
    "\n",
    "# --- 2. Prepare Data for Model A (Draw-Finder) ---\n",
    "y_is_draw = np.where(y_multi_class == 'D', 1, 0)\n",
    "X_train, X_test, y_train_is_draw, y_test_is_draw = train_test_split(X, y_is_draw, test_size=0.2, random_state=42, stratify=y_is_draw)\n",
    "\n",
    "# --- 3. Train Model A (Draw-Finder) ---\n",
    "print(\"\\nTraining Model A (Draw-Finder)...\")\n",
    "scale_pos_weight_draw = np.sum(y_train_is_draw == 0) / np.sum(y_train_is_draw == 1)\n",
    "draw_finder_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42, scale_pos_weight=scale_pos_weight_draw)\n",
    "draw_finder_model.fit(X_train, y_train_is_draw)\n",
    "\n",
    "# --- 4. Prepare Data for Model B (Winner-Picker) ---\n",
    "# Filter the original dataset to only include non-draws\n",
    "non_draw_df = df_final[df_final['FTR'] != 'D'].copy()\n",
    "X_non_draw = non_draw_df[final_feature_list].copy()\n",
    "y_non_draw = non_draw_df['FTR']\n",
    "\n",
    "# Preprocess for the second model\n",
    "X_non_draw['HomeTeam'] = LabelEncoder().fit_transform(X_non_draw['HomeTeam'])\n",
    "X_non_draw['AwayTeam'] = LabelEncoder().fit_transform(X_non_draw['AwayTeam'])\n",
    "y_non_draw_encoded = LabelEncoder().fit_transform(y_non_draw) # H=1, A=0\n",
    "\n",
    "# --- 5. Train Model B (Winner-Picker) ---\n",
    "print(\"\\nTraining Model B (Winner-Picker)...\")\n",
    "winner_picker_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42)\n",
    "winner_picker_model.fit(X_non_draw, y_non_draw_encoded)\n",
    "\n",
    "# --- 6. Execute Full Pipeline on the Test Set ---\n",
    "print(\"\\nExecuting full prediction pipeline on the test set...\")\n",
    "# First, get predictions from the Draw-Finder\n",
    "draw_preds = draw_finder_model.predict(X_test)\n",
    "\n",
    "# Get predictions from the Winner-Picker for all test samples\n",
    "# (We'll only use them if the Draw-Finder says \"Not-Draw\")\n",
    "winner_preds_raw = winner_picker_model.predict(X_test)\n",
    "\n",
    "# Reconstruct the final multi-class predictions\n",
    "final_preds = []\n",
    "for i in range(len(X_test)):\n",
    "    if draw_preds[i] == 1:\n",
    "        final_preds.append('D') # Draw-Finder predicts a Draw\n",
    "    else:\n",
    "        # Winner-Picker decides between Home and Away\n",
    "        if winner_preds_raw[i] == 1: # Assuming H was encoded as 1\n",
    "            final_preds.append('H')\n",
    "        else:\n",
    "            final_preds.append('A')\n",
    "\n",
    "# --- 7. Final System Evaluation ---\n",
    "# We need to compare our final_preds ('A','D','H') with the original multi-class test labels\n",
    "y_test_multi_class = LabelEncoder().fit_transform(y_multi_class) # Ensure it's encoded for splitting\n",
    "_, y_test_original_labels = train_test_split(y_multi_class, test_size=0.2, random_state=42, stratify=y_is_draw)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test_original_labels, final_preds)\n",
    "print(f\"\\n-------------------------------------------------\")\n",
    "print(f\"Final 'Draw Specialist' System Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"-------------------------------------------------\")\n",
    "print(\"\\nFinal System Classification Report:\")\n",
    "print(classification_report(y_test_original_labels, final_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
