{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5e42d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6589267",
   "metadata": {},
   "source": [
    "Data is present in Epl/data/ and seperate features required are taken to reduce the difficulties in cleaning..empty coloumns if any are filled with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fef55a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Epl/data\\\\E02000.csv', 'C:/Epl/data\\\\E02001.csv', 'C:/Epl/data\\\\E02002.csv', 'C:/Epl/data\\\\E02003.csv', 'C:/Epl/data\\\\E02004.csv', 'C:/Epl/data\\\\E02005.csv', 'C:/Epl/data\\\\E02006.csv', 'C:/Epl/data\\\\E02007.csv', 'C:/Epl/data\\\\E02008.csv', 'C:/Epl/data\\\\E02009.csv', 'C:/Epl/data\\\\E02010.csv', 'C:/Epl/data\\\\E02011.csv', 'C:/Epl/data\\\\E02012.csv', 'C:/Epl/data\\\\E02013.csv', 'C:/Epl/data\\\\E02014.csv', 'C:/Epl/data\\\\E02015.csv', 'C:/Epl/data\\\\E02016.csv', 'C:/Epl/data\\\\E02017.csv', 'C:/Epl/data\\\\E02018.csv', 'C:/Epl/data\\\\E02019.csv', 'C:/Epl/data\\\\E02020.csv', 'C:/Epl/data\\\\E02021.csv', 'C:/Epl/data\\\\E02022.csv', 'C:/Epl/data\\\\E02023.csv', 'C:/Epl/data\\\\E02024.csv']\n",
      "Shape: (9411, 18)\n",
      "        Date  HomeTeam       AwayTeam  HTHG  HTAG    HS    AS   HST  AST  \\\n",
      "0 2000-08-19  Charlton       Man City   2.0   0.0  17.0   8.0  14.0  4.0   \n",
      "1 2000-08-19   Chelsea       West Ham   1.0   0.0  17.0  12.0  10.0  5.0   \n",
      "2 2000-08-19  Coventry  Middlesbrough   1.0   1.0   6.0  16.0   3.0  9.0   \n",
      "3 2000-08-19     Derby    Southampton   1.0   2.0   6.0  13.0   4.0  6.0   \n",
      "4 2000-08-19     Leeds        Everton   2.0   0.0  17.0  12.0   8.0  6.0   \n",
      "\n",
      "     HF    AF   HC   AC   HY   AY   HR   AR FTR  \n",
      "0  13.0  12.0  6.0  6.0  1.0  2.0  0.0  0.0   H  \n",
      "1  19.0  14.0  7.0  7.0  1.0  2.0  0.0  0.0   H  \n",
      "2  15.0  21.0  8.0  4.0  5.0  3.0  1.0  0.0   A  \n",
      "3  11.0  13.0  5.0  8.0  1.0  1.0  0.0  0.0   D  \n",
      "4  21.0  20.0  6.0  4.0  1.0  3.0  0.0  0.0   H  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnas\\AppData\\Local\\Temp\\ipykernel_25568\\2351140533.py:16: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Date\"] = pd.to_datetime(data[\"Date\"], dayfirst=True, errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "# Load only Premier League CSVs (E0)\n",
    "files = glob.glob(\"C:/Epl/data/E0*.csv\")\n",
    "print(files)\n",
    "features=[\"Date\",\"HomeTeam\",\"AwayTeam\",\"HTHG\",\"HTAG\",\"HS\",\"AS\",\"HST\",\"AST\",\"HF\",\"AF\",\"HC\",\"AC\",\"HY\",\"AY\",\"HR\",\"AR\"]\n",
    "target =\"FTR\"\n",
    "dfs = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, encoding='cp1252', on_bad_lines='skip')\n",
    "    for col in features:\n",
    "        if col not in df.columns:\n",
    "         df[col] = pd.NA\n",
    "    df = df[features + [target]]\n",
    "    dfs.append(df)\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "print(\"Shape:\", data.shape)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc27b4",
   "metadata": {},
   "source": [
    "Now that data is ready we have to clean it by checking for duplicates and null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af76c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date        1\n",
      "HomeTeam    1\n",
      "AwayTeam    1\n",
      "HTHG        1\n",
      "HTAG        1\n",
      "HS          1\n",
      "AS          1\n",
      "HST         1\n",
      "AST         1\n",
      "HF          1\n",
      "AF          1\n",
      "HC          1\n",
      "AC          1\n",
      "HY          1\n",
      "AY          1\n",
      "HR          1\n",
      "AR          1\n",
      "FTR         1\n",
      "dtype: int64\n",
      "Date        0.01\n",
      "HomeTeam    0.01\n",
      "AwayTeam    0.01\n",
      "HTHG        0.01\n",
      "HTAG        0.01\n",
      "HS          0.01\n",
      "AS          0.01\n",
      "HST         0.01\n",
      "AST         0.01\n",
      "HF          0.01\n",
      "AF          0.01\n",
      "HC          0.01\n",
      "AC          0.01\n",
      "HY          0.01\n",
      "AY          0.01\n",
      "HR          0.01\n",
      "AR          0.01\n",
      "FTR         0.01\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())\n",
    "print((data.isnull().mean()*100).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21865d1d",
   "metadata": {},
   "source": [
    "This shows that the dataset has only a very few negligable output there is 1 missing value and that missing value exists in all features so we could drop that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08c89cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping missing: (9410, 18)\n",
      "Remaining NaN: 0\n"
     ]
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "print(\"After dropping missing:\",data.shape)\n",
    "print(\"Remaining NaN:\",data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0619b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "duplicates = data.duplicated().sum()\n",
    "print(\"Number of duplicated rows:\",duplicates)\n",
    "data.to_csv(\"match_winner_cleaned.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e63268c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Date  HomeTeam  AwayTeam  HTHG  HTAG  HS  AS  HST  AST  HF  AF  HC  AC  HY  \\\n",
      "0    56        13         8     0     0  14  10    5    2  12  10   7   8   2   \n",
      "1    58         9        11     0     0   7  18    2    5   9  18   2  10   3   \n",
      "2    58         0        19     1     0  18   9    6    3  17  14   8   2   2   \n",
      "3    58         7         4     0     1   9  10    1    5   8   8   1   5   1   \n",
      "4    58        14        16     1     0   3  19    1    4  15  16   3  12   2   \n",
      "\n",
      "   AY  HR  AR  FTR  \n",
      "0   3   0   0    2  \n",
      "1   1   0   0    0  \n",
      "2   2   0   0    2  \n",
      "3   1   1   0    0  \n",
      "4   4   1   0    2  \n"
     ]
    }
   ],
   "source": [
    "categorical_col = df.select_dtypes(include ='object').columns\n",
    "encoders = {}\n",
    "for c in categorical_col:\n",
    "    le = LabelEncoder()\n",
    "    df[c]=le.fit_transform(df[c])\n",
    "    encoders[c]=le\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e8c10",
   "metadata": {},
   "source": [
    "I have done label encoding for all categorical coloumns and the encoder function is also saved in encoders[] to decode for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bac3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into label and feature input\n",
    "\n",
    "y = df['FTR']\n",
    "\n",
    "X = df.drop('FTR', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "996a31e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372, 17)\n",
      "(372,)\n",
      "Class distribution after SMOTE: {np.int64(0): np.int64(124), np.int64(1): np.int64(124), np.int64(2): np.int64(124)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split data: 80% training, 20% testing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split to preserve class ratios\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_split, y_train_split)\n",
    "X_train_res = X_train_res.astype(np.float32)\n",
    "y_train_res = y_train_res.astype(np.int64)\n",
    "print(X_train_res.shape)\n",
    "print(y_train_res.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Class distribution after SMOTE:\", dict(zip(*np.unique(y_train_res, return_counts=True))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b13701",
   "metadata": {},
   "source": [
    "Random state fixed as 42 for accuracy and hyperparameter tuning and shufling is done "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82747ce3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '__sklearn_tags__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     11\u001b[39m param_grid = {\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m500\u001b[39m, \u001b[32m700\u001b[39m, \u001b[32m1000\u001b[39m],\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m6\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcolsample_bytree\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.7\u001b[39m, \u001b[32m0.8\u001b[39m, \u001b[32m0.9\u001b[39m]\n\u001b[32m     17\u001b[39m }\n\u001b[32m     18\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     19\u001b[39m     estimator=model,\n\u001b[32m     20\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     25\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m best_model = grid_search.best_estimator_\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest parameters:\u001b[39m\u001b[33m\"\u001b[39m, grid_search.best_params_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnas\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnas\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:933\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    929\u001b[39m params = _check_method_params(X, params=params)\n\u001b[32m    931\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._get_routed_params_for_fit(params)\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m cv_orig = check_cv(\u001b[38;5;28mself\u001b[39m.cv, y, classifier=\u001b[43mis_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    934\u001b[39m n_splits = cv_orig.get_n_splits(X, y, **routed_params.splitter.split)\n\u001b[32m    936\u001b[39m base_estimator = clone(\u001b[38;5;28mself\u001b[39m.estimator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnas\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1237\u001b[39m, in \u001b[36mis_classifier\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m   1230\u001b[39m     warnings.warn(\n\u001b[32m   1231\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpassing a class to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mprint\u001b[39m(inspect.stack()[\u001b[32m0\u001b[39m][\u001b[32m3\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1232\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in 1.8. Use an instance of the class instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1233\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1234\u001b[39m     )\n\u001b[32m   1235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m_estimator_type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m.estimator_type == \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnas\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_tags.py:430\u001b[39m, in \u001b[36mget_tags\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mtype\u001b[39m(estimator).mro()):\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m__sklearn_tags__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m         sklearn_tags_provider[klass] = \u001b[43mklass\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__sklearn_tags__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    431\u001b[39m         class_order.append(klass)\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_more_tags\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arnas\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:540\u001b[39m, in \u001b[36mClassifierMixin.__sklearn_tags__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__sklearn_tags__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     tags = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__sklearn_tags__\u001b[49m()\n\u001b[32m    541\u001b[39m     tags.estimator_type = \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    542\u001b[39m     tags.classifier_tags = ClassifierTags()\n",
      "\u001b[31mAttributeError\u001b[39m: 'super' object has no attribute '__sklearn_tags__'"
     ]
    }
   ],
   "source": [
    "weights = compute_sample_weight(class_weight='balanced', y=y_train_res)\n",
    "model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    objective='multi:softprob',  # for 3-class classification\n",
    "    num_class=3,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state = 42\n",
    ")\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 700, 1000],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',   # macro F1 helps balance all classes\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "#model.fit(X_train, y_train,sample_weight=weights)\n",
    "#y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79c541ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m f1_score\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Get predicted probabilities\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y_proba = \u001b[43mbest_model\u001b[49m.predict_proba(X_test_split)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Custom thresholds per class\u001b[39;00m\n\u001b[32m      7\u001b[39m thresholds = [\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m]  \u001b[38;5;66;03m# start with 0.5 for all classes\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_proba = best_model.predict_proba(X_test_split)\n",
    "\n",
    "# Custom thresholds per class\n",
    "thresholds = [0.5, 0.5, 0.5]  # start with 0.5 for all classes\n",
    "\n",
    "def predict_with_thresholds(probs, thresholds):\n",
    "    preds = []\n",
    "    for p in probs:\n",
    "        # Assign class if probability > threshold, else take max\n",
    "        assigned = [i for i, prob in enumerate(p) if prob >= thresholds[i]]\n",
    "        if assigned:\n",
    "            preds.append(assigned[0])\n",
    "        else:\n",
    "            preds.append(np.argmax(p))\n",
    "    return np.array(preds)\n",
    "\n",
    "y_pred = predict_with_thresholds(y_proba, thresholds)\n",
    "\n",
    "print(\"F1 score macro:\", f1_score(y_test_split, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f374b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.618421052631579\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.71        26\n",
      "           1       0.38      0.32      0.34        19\n",
      "           2       0.63      0.77      0.70        31\n",
      "\n",
      "    accuracy                           0.62        76\n",
      "   macro avg       0.59      0.58      0.58        76\n",
      "weighted avg       0.62      0.62      0.61        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Full classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
